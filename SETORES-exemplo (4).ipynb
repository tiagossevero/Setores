{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf87d1f-41bf-4de9-b0fd-c6bc402bd7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/poc\")\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/plugins\")\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/dags\")\n",
    "\n",
    "#Import libs python\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import date\n",
    "\n",
    "#Import libs internas\n",
    "from utils import spark_utils_session as utils\n",
    "\n",
    "from hooks.hdfs.hdfs_helper import HdfsHelper\n",
    "from jobs.job_base_config import BaseETLJobClass\n",
    "\n",
    "import poc_helper\n",
    "poc_helper.load_env(\"PROD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd280d8-e921-4131-8cc6-576c13b0558f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_session(profile: str, dynamic_allocation_enabled: bool = True) -> utils.DBASparkAppSession:\n",
    "    \"\"\"Generates DBASparkAppSession.\"\"\"\n",
    "    \n",
    "    app_name = \"tsevero_setores\"\n",
    "    \n",
    "    \n",
    "    spark_builder = (utils.DBASparkAppSession\n",
    "                     .builder\n",
    "                     .setAppName(app_name)\n",
    "                     .usingProcessProfile(profile)\n",
    "                    )\n",
    "    \n",
    "    if dynamic_allocation_enabled:\n",
    "        spark_builder.autoResourceManagement()\n",
    "\n",
    "    return spark_builder.build()\n",
    "\n",
    "session = get_session(profile='efd_t2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499ff0ec-11ee-4df0-ae15-454dc64a609c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "session.sparkSession.sql(\"SHOW DATABASES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7785b663-d61c-4f77-a94a-2afa471b48a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SISTEMA DE AN√ÅLISE TRIBUT√ÅRIA SETORIAL v4.0 - AN√ÅLISE COMPLETA COM ML\n",
    "# Receita Estadual de Santa Catarina\n",
    "# =============================================================================\n",
    "# Este script realiza an√°lises avan√ßadas usando Spark SQL para processamento\n",
    "# e bibliotecas Python (Pandas, Matplotlib, Seaborn, Plotly, Scikit-learn) \n",
    "# para visualiza√ß√µes e machine learning.\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "\n",
    "# Sklearn para ML\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "# =============================================================================\n",
    "# PREVEN√á√ÉO DE CONFLITOS: Python Built-ins vs PySpark\n",
    "# =============================================================================\n",
    "import builtins\n",
    "import numpy as np\n",
    "\n",
    "# Salvar refer√™ncias √†s fun√ß√µes built-in que conflitam com PySpark\n",
    "max_builtin = builtins.max\n",
    "min_builtin = builtins.min\n",
    "abs_builtin = builtins.abs\n",
    "sum_builtin = builtins.sum\n",
    "round_builtin = builtins.round\n",
    "\n",
    "print(\"‚úÖ Fun√ß√µes built-in protegidas contra conflitos com PySpark\")\n",
    "print(\"   Use: max_builtin(), min_builtin(), abs_builtin(), sum_builtin(), round_builtin()\")\n",
    "print(\"   Ou use as vers√µes do numpy: np.abs(), np.max(), np.min(), etc.\")\n",
    "\n",
    "# Configura√ß√µes visuais\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SISTEMA DE AN√ÅLISE TRIBUT√ÅRIA SETORIAL v4.0 - AN√ÅLISE COMPLETA COM ML\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Iniciado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Verificar sess√£o Spark\n",
    "try:\n",
    "    if 'session' in locals():\n",
    "        spark = session.sparkSession\n",
    "        print(f\"‚úÖ Sess√£o Spark dispon√≠vel: {spark.sparkContext.appName}\")\n",
    "    elif 'spark' in locals():\n",
    "        print(f\"‚úÖ Sess√£o Spark dispon√≠vel\")\n",
    "    else:\n",
    "        raise NameError(\"Sess√£o Spark n√£o encontrada.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERRO: {e}\")\n",
    "    print(\"Execute primeiro o c√≥digo de configura√ß√£o da sess√£o Spark.\")\n",
    "    raise\n",
    "\n",
    "# =============================================================================\n",
    "# PARTE 1: CARREGAMENTO E PREPARA√á√ÉO DOS DADOS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PARTE 1: CARREGAMENTO E PREPARA√á√ÉO DOS DADOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Carregar dados das views principais\n",
    "print(\"\\nüìä Carregando dados das views do sistema...\")\n",
    "\n",
    "try:\n",
    "    # Benchmark Setorial\n",
    "    df_benchmark = spark.table(\"niat.argos_benchmark_setorial\")\n",
    "    total_benchmark = df_benchmark.count()\n",
    "    print(f\"‚úÖ Benchmark Setorial: {total_benchmark:,} registros\")\n",
    "    \n",
    "    # Empresas vs Benchmark\n",
    "    df_empresas = spark.table(\"niat.argos_empresa_vs_benchmark\")\n",
    "    total_empresas = df_empresas.count()\n",
    "    print(f\"‚úÖ Empresas vs Benchmark: {total_empresas:,} registros\")\n",
    "    \n",
    "    # Evolu√ß√£o Temporal Setorial\n",
    "    df_evolucao_setor = spark.table(\"niat.argos_evolucao_temporal_setor\")\n",
    "    total_evo_setor = df_evolucao_setor.count()\n",
    "    print(f\"‚úÖ Evolu√ß√£o Setorial: {total_evo_setor:,} setores\")\n",
    "    \n",
    "    # Alertas\n",
    "    df_alertas = spark.table(\"niat.argos_alertas_empresas\")\n",
    "    total_alertas = df_alertas.count()\n",
    "    print(f\"‚úÖ Alertas: {total_alertas:,} registros\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERRO ao carregar dados: {e}\")\n",
    "    print(\"Certifique-se de que o script SQL v4.0 foi executado completamente.\")\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd28fc2-06c9-4191-aaa7-b0bc46bae4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PARTE 2: AN√ÅLISE EXPLORAT√ìRIA E ESTAT√çSTICAS DESCRITIVAS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PARTE 2: AN√ÅLISE EXPLORAT√ìRIA E ESTAT√çSTICAS DESCRITIVAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 2.1. Estat√≠sticas Gerais do Sistema\n",
    "print(\"\\nüìà 2.1. ESTAT√çSTICAS GERAIS DO SISTEMA\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "stats_gerais = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(DISTINCT b.cnae_classe) AS total_setores,\n",
    "    COUNT(DISTINCT e.nu_cnpj) AS total_empresas,\n",
    "    COUNT(DISTINCT e.nu_per_ref) AS total_periodos,\n",
    "    ROUND(SUM(e.vl_faturamento) / 1e9, 2) AS faturamento_total_bilhoes,\n",
    "    ROUND(SUM(e.icms_devido) / 1e9, 2) AS icms_devido_bilhoes,\n",
    "    ROUND(AVG(e.aliq_efetiva) * 100, 2) AS aliq_media_sistema_pct,\n",
    "    COUNT(DISTINCT CASE WHEN a.severidade = 'CRITICO' THEN e.nu_cnpj END) AS empresas_risco_critico\n",
    "FROM niat.argos_empresas e\n",
    "LEFT JOIN niat.argos_benchmark_setorial b ON e.cnae_classe = b.cnae_classe AND e.nu_per_ref = b.nu_per_ref\n",
    "LEFT JOIN niat.argos_alertas_empresas a ON e.nu_cnpj = a.nu_cnpj AND e.nu_per_ref = a.nu_per_ref\n",
    "WHERE e.nu_per_ref = (SELECT MAX(nu_per_ref) FROM niat.argos_empresas)\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(\"\\nM√âTRICAS PRINCIPAIS:\")\n",
    "for col in stats_gerais.columns:\n",
    "    valor = stats_gerais[col].iloc[0]\n",
    "    print(f\"  ‚Ä¢ {col}: {valor:,.2f}\" if isinstance(valor, float) else f\"  ‚Ä¢ {col}: {valor:,}\")\n",
    "\n",
    "# 2.2. Distribui√ß√£o por Porte\n",
    "print(\"\\nüìä 2.2. DISTRIBUI√á√ÉO POR PORTE EMPRESARIAL\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "dist_porte = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    porte_empresa,\n",
    "    COUNT(DISTINCT nu_cnpj) AS qtd_empresas,\n",
    "    ROUND(AVG(vl_faturamento), 2) AS faturamento_medio,\n",
    "    ROUND(AVG(aliq_efetiva) * 100, 2) AS aliq_media_pct\n",
    "FROM niat.argos_empresas\n",
    "WHERE nu_per_ref = (SELECT MAX(nu_per_ref) FROM niat.argos_empresas)\n",
    "  AND porte_empresa != 'SEM_FATURAMENTO'\n",
    "GROUP BY porte_empresa\n",
    "ORDER BY \n",
    "    CASE porte_empresa\n",
    "        WHEN 'MICRO' THEN 1\n",
    "        WHEN 'PEQUENO' THEN 2\n",
    "        WHEN 'MEDIO' THEN 3\n",
    "        WHEN 'GRANDE' THEN 4\n",
    "    END\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(dist_porte.to_string(index=False))\n",
    "\n",
    "# Visualiza√ß√£o: Distribui√ß√£o por Porte\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "fig.suptitle('Distribui√ß√£o por Porte Empresarial', fontsize=16, fontweight='bold')\n",
    "\n",
    "sns.barplot(data=dist_porte, x='porte_empresa', y='qtd_empresas', ax=ax1, palette='viridis')\n",
    "ax1.set_title('Quantidade de Empresas por Porte')\n",
    "ax1.set_xlabel('Porte')\n",
    "ax1.set_ylabel('N√∫mero de Empresas')\n",
    "ax1.bar_label(ax1.containers[0], fmt='%d')\n",
    "\n",
    "sns.barplot(data=dist_porte, x='porte_empresa', y='aliq_media_pct', ax=ax2, palette='plasma')\n",
    "ax2.set_title('Al√≠quota M√©dia por Porte (%)')\n",
    "ax2.set_xlabel('Porte')\n",
    "ax2.set_ylabel('Al√≠quota M√©dia (%)')\n",
    "ax2.bar_label(ax2.containers[0], fmt='%.2f%%')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f218001f-17d7-474d-81fa-3329c2668ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PARTE 3: VISUALIZA√á√ïES AVAN√áADAS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PARTE 3: VISUALIZA√á√ïES AVAN√áADAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 3.1. Evolu√ß√£o Temporal por Setor (Top 10)\n",
    "print(\"\\nüìà 3.1. EVOLU√á√ÉO TEMPORAL DOS TOP 10 SETORES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "evolucao_top_setores = spark.sql(\"\"\"\n",
    "WITH top_setores AS (\n",
    "    SELECT cnae_classe\n",
    "    FROM niat.argos_evolucao_temporal_setor\n",
    "    ORDER BY faturamento_acumulado_8m DESC\n",
    "    LIMIT 10\n",
    ")\n",
    "SELECT \n",
    "    b.nu_per_ref,\n",
    "    b.cnae_classe,\n",
    "    b.desc_cnae_classe,\n",
    "    ROUND(b.aliq_efetiva_mediana * 100, 2) AS aliq_mediana_pct,\n",
    "    ROUND(b.faturamento_total / 1e6, 2) AS faturamento_milhoes\n",
    "FROM niat.argos_benchmark_setorial b\n",
    "INNER JOIN top_setores ts ON b.cnae_classe = ts.cnae_classe\n",
    "ORDER BY b.cnae_classe, b.nu_per_ref\n",
    "\"\"\").toPandas()\n",
    "\n",
    "# Converter per√≠odo para datetime\n",
    "evolucao_top_setores['periodo_dt'] = pd.to_datetime(evolucao_top_setores['nu_per_ref'], format='%Y%m')\n",
    "\n",
    "# Converter Decimal para float\n",
    "evolucao_top_setores['aliq_mediana_pct'] = evolucao_top_setores['aliq_mediana_pct'].astype(float)\n",
    "evolucao_top_setores['faturamento_milhoes'] = evolucao_top_setores['faturamento_milhoes'].astype(float)\n",
    "\n",
    "# Gr√°fico interativo com Plotly\n",
    "fig = px.line(evolucao_top_setores, \n",
    "              x='periodo_dt', \n",
    "              y='aliq_mediana_pct',\n",
    "              color='desc_cnae_classe',\n",
    "              title='Evolu√ß√£o Temporal das Al√≠quotas Medianas - Top 10 Setores',\n",
    "              labels={'periodo_dt': 'Per√≠odo', \n",
    "                     'aliq_mediana_pct': 'Al√≠quota Mediana (%)',\n",
    "                     'desc_cnae_classe': 'Setor'},\n",
    "              hover_data=['faturamento_milhoes'])\n",
    "fig.update_layout(height=600, hovermode='x unified')\n",
    "fig.show()\n",
    "\n",
    "# 3.2. Mapa de Calor: Al√≠quotas por CNAE e Per√≠odo\n",
    "print(\"\\nüî• 3.2. MAPA DE CALOR: AL√çQUOTAS POR CNAE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "heatmap_data = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    cnae_classe,\n",
    "    desc_cnae_classe,\n",
    "    nu_per_ref,\n",
    "    ROUND(aliq_efetiva_mediana * 100, 2) AS aliq_mediana_pct\n",
    "FROM niat.argos_benchmark_setorial\n",
    "WHERE cnae_classe IN (\n",
    "    SELECT cnae_classe \n",
    "    FROM niat.argos_evolucao_temporal_setor \n",
    "    ORDER BY faturamento_acumulado_8m DESC \n",
    "    LIMIT 15\n",
    ")\n",
    "ORDER BY cnae_classe, nu_per_ref\n",
    "\"\"\").toPandas()\n",
    "\n",
    "# Pivotar dados para matriz\n",
    "heatmap_pivot = heatmap_data.pivot(index='desc_cnae_classe', columns='nu_per_ref', values='aliq_mediana_pct')\n",
    "\n",
    "# Converter Decimal para float\n",
    "heatmap_pivot = heatmap_pivot.astype(float)\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.heatmap(heatmap_pivot, annot=True, fmt='.1f', cmap='YlOrRd', linewidths=0.5, cbar_kws={'label': 'Al√≠quota Mediana (%)'})\n",
    "plt.title('Mapa de Calor: Al√≠quotas Medianas por Setor e Per√≠odo', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Per√≠odo')\n",
    "plt.ylabel('Setor (CNAE)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3.3. Distribui√ß√£o de Empresas por Quartil de Al√≠quota\n",
    "print(\"\\nüìä 3.3. DISTRIBUI√á√ÉO POR QUARTIL DE AL√çQUOTA\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "quartis_data = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    CASE \n",
    "        WHEN aliq_efetiva_empresa < aliq_setor_p25 THEN 'Q1 (Abaixo P25)'\n",
    "        WHEN aliq_efetiva_empresa < aliq_setor_mediana THEN 'Q2 (P25-P50)'\n",
    "        WHEN aliq_efetiva_empresa < aliq_setor_p75 THEN 'Q3 (P50-P75)'\n",
    "        ELSE 'Q4 (Acima P75)'\n",
    "    END AS quartil,\n",
    "    COUNT(*) AS qtd_empresas,\n",
    "    ROUND(AVG(vl_faturamento) / 1e6, 2) AS faturamento_medio_milhoes\n",
    "FROM niat.argos_empresa_vs_benchmark\n",
    "WHERE nu_per_ref = (SELECT MAX(nu_per_ref) FROM niat.argos_empresa_vs_benchmark)\n",
    "  AND aliq_efetiva_empresa IS NOT NULL\n",
    "GROUP BY 1\n",
    "ORDER BY 1\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(quartis_data.to_string(index=False))\n",
    "\n",
    "# Converter Decimal para float\n",
    "quartis_data['qtd_empresas'] = quartis_data['qtd_empresas'].astype(int)\n",
    "quartis_data['faturamento_medio_milhoes'] = quartis_data['faturamento_medio_milhoes'].astype(float)\n",
    "\n",
    "# Visualiza√ß√£o\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "x = np.arange(len(quartis_data))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, quartis_data['qtd_empresas'], width, label='Quantidade', color='steelblue')\n",
    "ax2 = ax.twinx()\n",
    "bars2 = ax2.bar(x + width/2, quartis_data['faturamento_medio_milhoes'], width, label='Fat. M√©dio', color='coral')\n",
    "\n",
    "ax.set_xlabel('Quartil de Al√≠quota')\n",
    "ax.set_ylabel('Quantidade de Empresas', color='steelblue')\n",
    "ax2.set_ylabel('Faturamento M√©dio (Milh√µes R$)', color='coral')\n",
    "ax.set_title('Distribui√ß√£o de Empresas por Quartil de Al√≠quota', fontsize=16, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(quartis_data['quartil'])\n",
    "ax.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "ax.bar_label(bars1, fmt='%d')\n",
    "ax2.bar_label(bars2, fmt='%.1f')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3.4. An√°lise de Diverg√™ncias ICMS vs Pagamentos\n",
    "print(\"\\nüí∞ 3.4. AN√ÅLISE DE DIVERG√äNCIAS ICMS vs PAGAMENTOS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "divergencias = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    CASE\n",
    "        WHEN icms_recolher > 100 AND valor_total_pago = 0 THEN 'Sem Pagamento'\n",
    "        WHEN ABS(icms_recolher - valor_total_pago) / NULLIF(icms_recolher, 0) > 0.5 THEN 'Diverg√™ncia Extrema (>50%)'\n",
    "        WHEN ABS(icms_recolher - valor_total_pago) / NULLIF(icms_recolher, 0) > 0.3 THEN 'Diverg√™ncia Alta (30-50%)'\n",
    "        WHEN ABS(icms_recolher - valor_total_pago) / NULLIF(icms_recolher, 0) > 0.1 THEN 'Diverg√™ncia M√©dia (10-30%)'\n",
    "        ELSE 'Normal (<10%)'\n",
    "    END AS tipo_divergencia,\n",
    "    COUNT(*) AS qtd_empresas,\n",
    "    ROUND(SUM(icms_recolher) / 1e6, 2) AS icms_total_milhoes,\n",
    "    ROUND(SUM(valor_total_pago) / 1e6, 2) AS pagto_total_milhoes\n",
    "FROM niat.argos_empresa_vs_benchmark\n",
    "WHERE nu_per_ref = (SELECT MAX(nu_per_ref) FROM niat.argos_empresa_vs_benchmark)\n",
    "  AND icms_recolher IS NOT NULL\n",
    "GROUP BY 1\n",
    "ORDER BY \n",
    "    CASE tipo_divergencia\n",
    "        WHEN 'Sem Pagamento' THEN 1\n",
    "        WHEN 'Diverg√™ncia Extrema (>50%)' THEN 2\n",
    "        WHEN 'Diverg√™ncia Alta (30-50%)' THEN 3\n",
    "        WHEN 'Diverg√™ncia M√©dia (10-30%)' THEN 4\n",
    "        ELSE 5\n",
    "    END\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(divergencias.to_string(index=False))\n",
    "\n",
    "# Converter Decimal para float\n",
    "divergencias['icms_total_milhoes'] = divergencias['icms_total_milhoes'].astype(float)\n",
    "divergencias['pagto_total_milhoes'] = divergencias['pagto_total_milhoes'].astype(float)\n",
    "\n",
    "# Visualiza√ß√£o com Plotly\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(name='ICMS a Recolher', x=divergencias['tipo_divergencia'], y=divergencias['icms_total_milhoes'], marker_color='indianred'))\n",
    "fig.add_trace(go.Bar(name='Pagamentos Realizados', x=divergencias['tipo_divergencia'], y=divergencias['pagto_total_milhoes'], marker_color='lightseagreen'))\n",
    "fig.update_layout(\n",
    "    title='An√°lise de Diverg√™ncias: ICMS a Recolher vs Pagamentos Realizados',\n",
    "    xaxis_title='Tipo de Diverg√™ncia',\n",
    "    yaxis_title='Valor Total (Milh√µes R$)',\n",
    "    barmode='group',\n",
    "    height=600\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# 3.5. Ranking de Setores com Anomalias\n",
    "print(\"\\n‚ö†Ô∏è 3.5. RANKING DE SETORES COM ANOMALIAS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "anomalias_ranking = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    ans.cnae_classe,\n",
    "    ans.desc_cnae_classe,\n",
    "    ans.tipo_anomalia,\n",
    "    ans.severidade,\n",
    "    ROUND(ans.score_relevancia, 1) AS score,\n",
    "    ans.qtd_empresas_total,\n",
    "    ROUND(ans.faturamento_total / 1e9, 2) AS faturamento_bilhoes,\n",
    "    ROUND(ans.aliq_setor * 100, 2) AS aliq_setor_pct,\n",
    "    ROUND(ans.aliq_mediana_economia * 100, 2) AS aliq_economia_pct\n",
    "FROM niat.argos_anomalias_setoriais ans\n",
    "WHERE nu_per_ref = (SELECT MAX(nu_per_ref) FROM niat.argos_anomalias_setoriais)\n",
    "ORDER BY ans.score_relevancia DESC\n",
    "LIMIT 20\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(anomalias_ranking.to_string(index=False))\n",
    "\n",
    "# Converter Decimal para float\n",
    "anomalias_ranking['faturamento_bilhoes'] = anomalias_ranking['faturamento_bilhoes'].astype(float)\n",
    "anomalias_ranking['score'] = anomalias_ranking['score'].astype(float)\n",
    "anomalias_ranking['aliq_setor_pct'] = anomalias_ranking['aliq_setor_pct'].astype(float)\n",
    "\n",
    "# Visualiza√ß√£o\n",
    "fig = px.scatter(anomalias_ranking,\n",
    "                x='faturamento_bilhoes',\n",
    "                y='score',\n",
    "                size='qtd_empresas_total',\n",
    "                color='severidade',\n",
    "                hover_name='desc_cnae_classe',\n",
    "                hover_data=['tipo_anomalia', 'aliq_setor_pct'],\n",
    "                title='Setores com Anomalias: Score vs Faturamento',\n",
    "                labels={'faturamento_bilhoes': 'Faturamento (Bilh√µes R$)',\n",
    "                       'score': 'Score de Relev√¢ncia',\n",
    "                       'severidade': 'Severidade'},\n",
    "                color_discrete_map={'ALTA': '#d62728', 'MEDIA': '#ff7f0e', 'BAIXA': '#2ca02c'},\n",
    "                height=600)\n",
    "fig.update_xaxes(type='log')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce0b20f-1843-460e-8335-82624b800376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PARTE 4: AN√ÅLISES ESTAT√çSTICAS AVAN√áADAS COM MACHINE LEARNING\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PARTE 4: AN√ÅLISES ESTAT√çSTICAS AVAN√áADAS COM MACHINE LEARNING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 4.1. Clustering de Setores Similares\n",
    "print(\"\\nüî¨ 4.1. CLUSTERING DE SETORES SIMILARES (K-MEANS)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Preparar dados para clustering\n",
    "df_clustering = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    cnae_classe,\n",
    "    desc_cnae_classe,\n",
    "    aliq_mediana_media_8m,\n",
    "    coef_variacao_temporal,\n",
    "    faturamento_acumulado_8m,\n",
    "    icms_devido_acumulado_8m,\n",
    "    media_empresas_mes\n",
    "FROM niat.argos_evolucao_temporal_setor\n",
    "WHERE aliq_mediana_media_8m IS NOT NULL\n",
    "  AND coef_variacao_temporal IS NOT NULL\n",
    "\"\"\").toPandas()\n",
    "\n",
    "# Converter Decimal para float\n",
    "for col in df_clustering.select_dtypes(include=['object']).columns:\n",
    "    try:\n",
    "        df_clustering[col] = df_clustering[col].astype(float)\n",
    "    except:\n",
    "        pass  # Manter como string se n√£o for num√©rico\n",
    "\n",
    "# Features para clustering\n",
    "features_cluster = ['aliq_mediana_media_8m', 'coef_variacao_temporal', \n",
    "                   'faturamento_acumulado_8m', 'icms_devido_acumulado_8m', 'media_empresas_mes']\n",
    "\n",
    "# Normaliza√ß√£o\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_clustering[features_cluster].fillna(0))\n",
    "\n",
    "# Determinar n√∫mero √≥timo de clusters (m√©todo Elbow)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "\n",
    "# Visualizar m√©todo Elbow\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Determina√ß√£o do N√∫mero √ìtimo de Clusters', fontsize=16, fontweight='bold')\n",
    "\n",
    "ax1.plot(K_range, inertias, 'bo-')\n",
    "ax1.set_xlabel('N√∫mero de Clusters (k)')\n",
    "ax1.set_ylabel('In√©rcia')\n",
    "ax1.set_title('M√©todo Elbow')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(K_range, silhouette_scores, 'ro-')\n",
    "ax2.set_xlabel('N√∫mero de Clusters (k)')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Score por k')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Escolher k √≥timo (maior silhouette score) - CORRE√á√ÉO AQUI\n",
    "k_otimo = K_range[np.argmax(silhouette_scores)]\n",
    "max_silhouette = silhouette_scores[np.argmax(silhouette_scores)]  # Armazenar o valor antes\n",
    "print(f\"\\n‚úÖ N√∫mero √≥timo de clusters: {k_otimo} (Silhouette Score: {max_silhouette:.3f})\")\n",
    "\n",
    "# Aplicar K-Means com k √≥timo\n",
    "kmeans_final = KMeans(n_clusters=k_otimo, random_state=42, n_init=10)\n",
    "df_clustering['cluster'] = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "# An√°lise dos clusters\n",
    "print(f\"\\nCARACTER√çSTICAS DOS {k_otimo} CLUSTERS IDENTIFICADOS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for cluster_id in range(k_otimo):\n",
    "    cluster_data = df_clustering[df_clustering['cluster'] == cluster_id]\n",
    "    print(f\"\\nCluster {cluster_id} ({len(cluster_data)} setores):\")\n",
    "    print(f\"  Al√≠quota M√©dia: {cluster_data['aliq_mediana_media_8m'].mean():.4f}\")\n",
    "    print(f\"  Volatilidade M√©dia: {cluster_data['coef_variacao_temporal'].mean():.3f}\")\n",
    "    print(f\"  Faturamento M√©dio: R$ {cluster_data['faturamento_acumulado_8m'].mean()/1e9:.2f} bilh√µes\")\n",
    "    print(f\"  Setores representativos:\")\n",
    "    for setor in cluster_data.nlargest(3, 'faturamento_acumulado_8m')['desc_cnae_classe'].values:\n",
    "        print(f\"    ‚Ä¢ {setor}\")\n",
    "\n",
    "# Visualiza√ß√£o com PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df_clustering['cluster'], \n",
    "                     cmap='viridis', s=100, alpha=0.6, edgecolors='w', linewidth=0.5)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} vari√¢ncia)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} vari√¢ncia)')\n",
    "plt.title('Clustering de Setores Similares (K-Means + PCA)', fontsize=16, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4.2. Detec√ß√£o de Outliers por Setor (Isolation Forest)\n",
    "print(\"\\nüîç 4.2. DETEC√á√ÉO DE OUTLIERS POR SETOR (ISOLATION FOREST)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Preparar dados\n",
    "df_outliers = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    cnae_classe,\n",
    "    desc_cnae_classe,\n",
    "    aliq_mediana_media_8m,\n",
    "    coef_variacao_temporal,\n",
    "    LOG(faturamento_acumulado_8m + 1) AS log_faturamento,\n",
    "    aliq_mediana_max_8m - aliq_mediana_min_8m AS amplitude_aliq\n",
    "FROM niat.argos_evolucao_temporal_setor\n",
    "WHERE aliq_mediana_media_8m IS NOT NULL\n",
    "\"\"\").toPandas()\n",
    "\n",
    "# Converter Decimal para float\n",
    "for col in ['aliq_mediana_media_8m', 'coef_variacao_temporal', 'log_faturamento', 'amplitude_aliq']:\n",
    "    df_outliers[col] = df_outliers[col].astype(float)\n",
    "\n",
    "features_outliers = ['aliq_mediana_media_8m', 'coef_variacao_temporal', 'log_faturamento', 'amplitude_aliq']\n",
    "X_outliers = df_outliers[features_outliers].fillna(0)\n",
    "\n",
    "# Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "df_outliers['outlier'] = iso_forest.fit_predict(X_outliers)\n",
    "df_outliers['outlier_score'] = iso_forest.score_samples(X_outliers)\n",
    "\n",
    "outliers_detectados = df_outliers[df_outliers['outlier'] == -1].sort_values('outlier_score')\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è {len(outliers_detectados)} setores outliers detectados:\")\n",
    "print(\"-\" * 80)\n",
    "print(outliers_detectados[['desc_cnae_classe', 'aliq_mediana_media_8m', 'coef_variacao_temporal', 'outlier_score']].head(15).to_string(index=False))\n",
    "\n",
    "# Visualiza√ß√£o\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "normal = df_outliers[df_outliers['outlier'] == 1]\n",
    "outliers = df_outliers[df_outliers['outlier'] == -1]\n",
    "\n",
    "ax.scatter(normal['coef_variacao_temporal'], normal['aliq_mediana_media_8m'], \n",
    "          c='blue', label='Normal', alpha=0.6, s=80)\n",
    "ax.scatter(outliers['coef_variacao_temporal'], outliers['aliq_mediana_media_8m'], \n",
    "          c='red', label='Outlier', alpha=0.8, s=120, marker='^')\n",
    "\n",
    "ax.set_xlabel('Coeficiente de Varia√ß√£o Temporal')\n",
    "ax.set_ylabel('Al√≠quota Mediana M√©dia (8 meses)')\n",
    "ax.set_title('Detec√ß√£o de Setores Outliers (Isolation Forest)', fontsize=16, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4.3. Previs√£o de Tend√™ncias de Al√≠quotas (Regress√£o Linear)\n",
    "print(\"\\nüìà 4.3. PREVIS√ÉO DE TEND√äNCIAS DE AL√çQUOTAS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Selecionar top 5 setores para previs√£o\n",
    "top_5_setores = spark.sql(\"\"\"\n",
    "SELECT cnae_classe\n",
    "FROM niat.argos_evolucao_temporal_setor\n",
    "ORDER BY faturamento_acumulado_8m DESC\n",
    "LIMIT 5\n",
    "\"\"\").toPandas()['cnae_classe'].tolist()\n",
    "\n",
    "# Preparar dados de s√©ries temporais\n",
    "df_series = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    cnae_classe,\n",
    "    desc_cnae_classe,\n",
    "    nu_per_ref,\n",
    "    ROUND(aliq_efetiva_mediana * 100, 2) AS aliq_mediana_pct\n",
    "FROM niat.argos_benchmark_setorial\n",
    "WHERE cnae_classe IN ('{\"','\".join(top_5_setores)}')\n",
    "ORDER BY cnae_classe, nu_per_ref\n",
    "\"\"\").toPandas()\n",
    "\n",
    "# Converter para tipos corretos\n",
    "df_series['aliq_mediana_pct'] = df_series['aliq_mediana_pct'].astype(float)\n",
    "df_series['periodo_dt'] = pd.to_datetime(df_series['nu_per_ref'], format='%Y%m')\n",
    "df_series['periodo_num'] = (df_series['periodo_dt'] - df_series['periodo_dt'].min()).dt.days\n",
    "\n",
    "# Previs√£o para cada setor\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 14))\n",
    "fig.suptitle('Previs√£o de Tend√™ncias de Al√≠quotas - Top 5 Setores', fontsize=16, fontweight='bold')\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, setor in enumerate(top_5_setores):\n",
    "    df_setor = df_series[df_series['cnae_classe'] == setor].copy()\n",
    "    \n",
    "    # Treinar modelo\n",
    "    X = df_setor[['periodo_num']].values\n",
    "    y = df_setor['aliq_mediana_pct'].values\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Prever pr√≥ximos 6 meses\n",
    "    max_periodo = df_setor['periodo_num'].max()\n",
    "    periodos_futuros = np.arange(max_periodo, max_periodo + 180, 30).reshape(-1, 1)\n",
    "    previsoes = model.predict(periodos_futuros)\n",
    "    \n",
    "    # Plotar\n",
    "    ax = axes[idx]\n",
    "    ax.plot(df_setor['periodo_dt'], df_setor['aliq_mediana_pct'], 'o-', label='Hist√≥rico', linewidth=2)\n",
    "    \n",
    "    datas_futuras = pd.date_range(start=df_setor['periodo_dt'].max() + pd.DateOffset(months=1), periods=6, freq='MS')\n",
    "    ax.plot(datas_futuras, previsoes, 's--', label='Previs√£o', linewidth=2, color='red', alpha=0.7)\n",
    "    \n",
    "    ax.set_title(df_setor['desc_cnae_classe'].iloc[0][:40], fontsize=10)\n",
    "    ax.set_xlabel('Per√≠odo')\n",
    "    ax.set_ylabel('Al√≠quota Mediana (%)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # R¬≤ score\n",
    "    r2 = model.score(X, y)\n",
    "    ax.text(0.02, 0.98, f'R¬≤ = {r2:.3f}', transform=ax.transAxes, \n",
    "           verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Remover subplot extra\n",
    "if len(top_5_setores) < 6:\n",
    "    fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4.4. An√°lise de Correla√ß√£o entre Vari√°veis\n",
    "print(\"\\nüîó 4.4. AN√ÅLISE DE CORRELA√á√ÉO ENTRE VARI√ÅVEIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Preparar dados\n",
    "df_correlacao = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    aliq_mediana_media_8m AS aliq_media,\n",
    "    coef_variacao_temporal AS volatilidade,\n",
    "    LOG(faturamento_acumulado_8m + 1) AS log_faturamento,\n",
    "    LOG(icms_devido_acumulado_8m + 1) AS log_icms,\n",
    "    media_empresas_mes AS empresas\n",
    "FROM niat.argos_evolucao_temporal_setor\n",
    "WHERE aliq_mediana_media_8m IS NOT NULL\n",
    "\"\"\").toPandas()\n",
    "\n",
    "# Matriz de correla√ß√£o\n",
    "correlation_matrix = df_correlacao.corr()\n",
    "\n",
    "print(\"\\nMATRIZ DE CORRELA√á√ÉO:\")\n",
    "print(correlation_matrix.round(3))\n",
    "\n",
    "# Visualiza√ß√£o\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm', \n",
    "           center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Matriz de Correla√ß√£o entre Vari√°veis Setoriais', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correla√ß√µes mais fortes - CORRE√á√ÉO AQUI\n",
    "print(\"\\nCORRELA√á√ïES MAIS FORTES (|r| > 0.5):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Usar numpy.abs em vez da fun√ß√£o abs do Python (que est√° sendo sobrescrita pelo PySpark)\n",
    "import builtins\n",
    "abs_builtin = builtins.abs  # Salvar refer√™ncia √† fun√ß√£o built-in\n",
    "\n",
    "correlacoes_fortes = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        # Usar abs_builtin ou np.abs\n",
    "        if abs_builtin(corr_value) > 0.5:\n",
    "            correlacoes_fortes.append({\n",
    "                'var1': correlation_matrix.columns[i],\n",
    "                'var2': correlation_matrix.columns[j],\n",
    "                'correlacao': corr_value\n",
    "            })\n",
    "\n",
    "if len(correlacoes_fortes) > 0:\n",
    "    for item in correlacoes_fortes:\n",
    "        print(f\"  ‚Ä¢ {item['var1']} ‚Üî {item['var2']}: {item['correlacao']:.3f}\")\n",
    "else:\n",
    "    print(\"  Nenhuma correla√ß√£o forte (|r| > 0.5) detectada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0812bcc-b860-44e7-b720-67ded83085c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PARTE 5: AN√ÅLISES ESPEC√çFICAS ADICIONAIS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PARTE 5: AN√ÅLISES ESPEC√çFICAS ADICIONAIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 5.1. An√°lise de Vari√¢ncia entre Portes dentro do Setor\n",
    "print(\"\\nüìä 5.1. AN√ÅLISE DE VARI√ÇNCIA ENTRE PORTES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "variancia_porte = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    bp.cnae_classe,\n",
    "    b.desc_cnae_classe,\n",
    "    STDDEV(bp.aliq_efetiva_mediana) AS variancia_aliq_entre_portes,\n",
    "    MAX(bp.aliq_efetiva_mediana) - MIN(bp.aliq_efetiva_mediana) AS amplitude_aliq\n",
    "FROM niat.argos_benchmark_setorial_porte bp\n",
    "INNER JOIN niat.argos_benchmark_setorial b \n",
    "    ON bp.cnae_classe = b.cnae_classe AND bp.nu_per_ref = b.nu_per_ref\n",
    "WHERE bp.nu_per_ref = (SELECT MAX(nu_per_ref) FROM niat.argos_benchmark_setorial_porte)\n",
    "GROUP BY bp.cnae_classe, b.desc_cnae_classe\n",
    "HAVING COUNT(DISTINCT bp.porte_empresa) >= 3\n",
    "ORDER BY variancia_aliq_entre_portes DESC\n",
    "LIMIT 15\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(variancia_porte.to_string(index=False))\n",
    "\n",
    "# Visualiza√ß√£o\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(data=variancia_porte, y='desc_cnae_classe', x='variancia_aliq_entre_portes', palette='Reds_r')\n",
    "plt.title('Setores com Maior Vari√¢ncia de Al√≠quota entre Portes', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Desvio Padr√£o da Al√≠quota entre Portes')\n",
    "plt.ylabel('Setor')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5.2. Score de Risco Composto (ML-Enhanced)\n",
    "print(\"\\nüéØ 5.2. SCORE DE RISCO COMPOSTO (ML-ENHANCED)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calcular score usando m√∫ltiplos fatores\n",
    "df_score = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    e.nu_cnpj,\n",
    "    e.nm_razao_social,\n",
    "    e.cnae_classe,\n",
    "    e.porte_empresa,\n",
    "    e.aliq_efetiva_empresa,\n",
    "    e.aliq_setor_mediana,\n",
    "    e.status_vs_setor,\n",
    "    ete.categoria_volatilidade,\n",
    "    ete.aliq_coef_variacao_8m,\n",
    "    e.flag_divergencia_pagamento,\n",
    "    COALESCE(a.score_risco, 0) AS score_alerta\n",
    "FROM niat.argos_empresa_vs_benchmark e\n",
    "LEFT JOIN niat.argos_evolucao_temporal_empresa ete ON e.nu_cnpj = ete.nu_cnpj\n",
    "LEFT JOIN niat.argos_alertas_empresas a ON e.nu_cnpj = a.nu_cnpj AND e.nu_per_ref = a.nu_per_ref\n",
    "WHERE e.nu_per_ref = (SELECT MAX(nu_per_ref) FROM niat.argos_empresa_vs_benchmark)\n",
    "  AND e.aliq_efetiva_empresa IS NOT NULL\n",
    "\"\"\").toPandas()\n",
    "\n",
    "# Criar features para o modelo - CORRE√á√ÉO AQUI\n",
    "df_score['diferenca_aliq'] = abs_builtin(df_score['aliq_efetiva_empresa'] - df_score['aliq_setor_mediana'])\n",
    "# OU usando numpy (tamb√©m funciona):\n",
    "# df_score['diferenca_aliq'] = np.abs(df_score['aliq_efetiva_empresa'] - df_score['aliq_setor_mediana'])\n",
    "\n",
    "df_score['status_encoded'] = df_score['status_vs_setor'].map({\n",
    "    'MUITO_ABAIXO': 5, 'ABAIXO': 3, 'NORMAL': 1, 'ACIMA': 2, 'MUITO_ACIMA': 4, 'SEM_DADOS': 0\n",
    "})\n",
    "df_score['volatilidade_encoded'] = df_score['categoria_volatilidade'].map({\n",
    "    'ALTA': 3, 'MEDIA': 2, 'BAIXA': 1, 'SEM_DADOS': 0\n",
    "})\n",
    "\n",
    "# Random Forest para score composto\n",
    "features_rf = ['diferenca_aliq', 'status_encoded', 'volatilidade_encoded', \n",
    "              'aliq_coef_variacao_8m', 'flag_divergencia_pagamento', 'score_alerta']\n",
    "\n",
    "X_score = df_score[features_rf].fillna(0)\n",
    "\n",
    "# Criar target sint√©tico baseado em m√∫ltiplos crit√©rios\n",
    "df_score['risco_alto'] = (\n",
    "    (df_score['status_vs_setor'] == 'MUITO_ABAIXO') | \n",
    "    (df_score['volatilidade_encoded'] >= 2) |\n",
    "    (df_score['flag_divergencia_pagamento'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "y_score = df_score['risco_alto']\n",
    "\n",
    "# Treinar Random Forest\n",
    "rf_score = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)\n",
    "rf_score.fit(X_score, y_score)\n",
    "\n",
    "# Prever score\n",
    "df_score['score_ml'] = rf_score.predict(X_score) * 100\n",
    "\n",
    "# Top empresas por score ML\n",
    "top_score_ml = df_score.nlargest(20, 'score_ml')[['nm_razao_social', 'cnae_classe', 'porte_empresa', \n",
    "                                                    'status_vs_setor', 'categoria_volatilidade', 'score_ml']]\n",
    "\n",
    "print(\"\\nTOP 20 EMPRESAS POR SCORE ML:\")\n",
    "print(\"-\" * 80)\n",
    "print(top_score_ml.to_string(index=False))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features_rf,\n",
    "    'importance': rf_score.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=feature_importance, x='importance', y='feature', palette='viridis')\n",
    "plt.title('Import√¢ncia das Features no Score de Risco ML', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Import√¢ncia')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d053c87-c35e-4b11-b7e7-4beb052971e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PARTE 6: RESUMO EXECUTIVO E EXPORTA√á√ÉO\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PARTE 6: RESUMO EXECUTIVO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "resumo_executivo = f\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë           SISTEMA DE AN√ÅLISE TRIBUT√ÅRIA SETORIAL v4.0                     ‚ïë\n",
    "‚ïë                    RESUMO EXECUTIVO DA AN√ÅLISE                             ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "üìä M√âTRICAS GERAIS:\n",
    "  ‚Ä¢ Total de Setores Analisados: {stats_gerais['total_setores'].iloc[0]:,}\n",
    "  ‚Ä¢ Total de Empresas: {stats_gerais['total_empresas'].iloc[0]:,}\n",
    "  ‚Ä¢ Faturamento Total: R$ {stats_gerais['faturamento_total_bilhoes'].iloc[0]:.2f} bilh√µes\n",
    "  ‚Ä¢ ICMS Devido Total: R$ {stats_gerais['icms_devido_bilhoes'].iloc[0]:.2f} bilh√µes\n",
    "  ‚Ä¢ Al√≠quota M√©dia do Sistema: {stats_gerais['aliq_media_sistema_pct'].iloc[0]:.2f}%\n",
    "\n",
    "üî¨ AN√ÅLISES DE MACHINE LEARNING:\n",
    "  ‚Ä¢ Clusters de Setores Identificados: {k_otimo}\n",
    "  ‚Ä¢ Setores Outliers Detectados: {len(outliers_detectados)}\n",
    "  ‚Ä¢ Empresas com Alto Risco (ML): {(df_score['score_ml'] > 70).sum():,}\n",
    "\n",
    "‚ö†Ô∏è ALERTAS CR√çTICOS:\n",
    "  ‚Ä¢ Empresas em Risco Cr√≠tico: {stats_gerais['empresas_risco_critico'].iloc[0]:,}\n",
    "  ‚Ä¢ Setores com Anomalias: {len(anomalias_ranking)}\n",
    "  ‚Ä¢ Diverg√™ncias ICMS vs Pagamentos: {divergencias['qtd_empresas'].sum():,} empresas\n",
    "\n",
    "üìà TEND√äNCIAS IDENTIFICADAS:\n",
    "  ‚Ä¢ Setores com Alta Volatilidade: {(df_clustering['coef_variacao_temporal'] > 0.3).sum()}\n",
    "  ‚Ä¢ Correla√ß√£o Faturamento-ICMS: {correlation_matrix.loc['log_faturamento', 'log_icms']:.3f}\n",
    "\n",
    "üí° RECOMENDA√á√ïES:\n",
    "  1. Priorizar fiscaliza√ß√£o dos {len(outliers_detectados)} setores outliers\n",
    "  2. Monitorar empresas com score ML > 70 ({(df_score['score_ml'] > 70).sum():,} casos)\n",
    "  3. Investigar diverg√™ncias ICMS em {divergencias.iloc[0]['qtd_empresas']} empresas\n",
    "  4. Analisar clusters de setores para a√ß√µes setoriais coordenadas\n",
    "\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "An√°lise conclu√≠da em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\"\"\"\n",
    "\n",
    "print(resumo_executivo)\n",
    "\n",
    "# Salvar resumo em arquivo (opcional)\n",
    "# with open('/tmp/argos_resumo_executivo.txt', 'w', encoding='utf-8') as f:\n",
    "#     f.write(resumo_executivo)\n",
    "# print(\"\\n‚úÖ Resumo executivo salvo em: /tmp/argos_resumo_executivo.txt\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ AN√ÅLISE COMPLETA FINALIZADA COM SUCESSO!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nPR√ìXIMOS PASSOS:\")\n",
    "print(\"  1. Utilizar os DataFrames gerados para an√°lises espec√≠ficas\")\n",
    "print(\"  2. Exportar resultados para dashboards (Streamlit/PowerBI)\")\n",
    "print(\"  3. Implementar sistema de monitoramento cont√≠nuo\")\n",
    "print(\"  4. Integrar com sistema de alertas autom√°ticos\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Data Pipeline)",
   "language": "python",
   "name": "conda_data_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

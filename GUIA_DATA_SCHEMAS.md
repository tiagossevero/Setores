# ğŸš€ Guia RÃ¡pido - GeraÃ§Ã£o de Data Schemas

## O que foi criado

Criei um sistema completo para gerar automaticamente os data-schemas de todas as tabelas do projeto ARGOS SETORES.

## ğŸ“¦ Arquivos criados

```
Setores/
â”œâ”€â”€ GERAR_DATA_SCHEMAS.ipynb          # Notebook principal (USE ESTE!)
â”œâ”€â”€ GUIA_DATA_SCHEMAS.md              # Este arquivo
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ gerar_data_schemas.py         # Script Python standalone
â””â”€â”€ data-schema/
    â”œâ”€â”€ README.md                      # DocumentaÃ§Ã£o da estrutura
    â”œâ”€â”€ EXEMPLO_OUTPUT.md              # Exemplo de schema gerado
    â”œâ”€â”€ originais/                     # Schemas das tabelas ODS (serÃ¡ criado)
    â”œâ”€â”€ intermediarias/                # Schemas das tabelas NIAT (serÃ¡ criado)
    â””â”€â”€ views/                         # Schemas das views (serÃ¡ criado)
```

## ğŸ¯ Como usar

### MÃ©todo Recomendado: Jupyter Notebook

1. **Abra o notebook:**
   ```bash
   jupyter notebook GERAR_DATA_SCHEMAS.ipynb
   ```

2. **Execute as cÃ©lulas em ordem:**
   - **CÃ©lula 1:** Configura a sessÃ£o Spark
   - **CÃ©lula 2:** Carrega as funÃ§Ãµes do gerador
   - **CÃ©lula 3:** Executa a geraÃ§Ã£o (escolha OpÃ§Ã£o A ou B)
   - **CÃ©lula 4:** Lista os arquivos gerados

3. **Resultado:**
   - Arquivos `.md` serÃ£o criados em `./data-schema/`
   - Cada arquivo contÃ©m DESCRIBE FORMATTED + SELECT LIMIT 10

## ğŸ“‹ Tabelas que serÃ£o documentadas

### Originais (3 tabelas)
```
usr_sat_ods.vw_ods_decl_dime       # DeclaraÃ§Ãµes ICMS
usr_sat_ods.vw_ods_contrib         # Cadastro de contribuintes
usr_sat_ods.vw_ods_pagamento       # Pagamentos ICMS
```

### IntermediÃ¡rias (9 tabelas)
```
niat.argos_benchmark_setorial           # Benchmark mensal
niat.argos_benchmark_setorial_porte     # Benchmark por porte
niat.argos_empresas                     # Dados empresas
niat.argos_pagamentos_empresa           # AgregaÃ§Ã£o pagamentos
niat.argos_empresa_vs_benchmark         # ComparaÃ§Ã£o empresa/setor
niat.argos_evolucao_temporal_setor      # TendÃªncias setoriais
niat.argos_evolucao_temporal_empresa    # TendÃªncias empresas
niat.argos_anomalias_setoriais          # Anomalias
niat.argos_alertas_empresas             # Alertas
```

### Views Auxiliares (4 views - opcional)
```
niat.vw_dashboard_setores
niat.vw_dashboard_empresas
niat.vw_analise_volatilidade
niat.vw_relacao_icms_pagamentos
```

## âš™ï¸ OpÃ§Ãµes de ExecuÃ§Ã£o

### OpÃ§Ã£o A: Sem Views (Recomendado)
- Gera apenas tabelas originais + intermediÃ¡rias
- Total: 12 tabelas
- Tempo estimado: 5-10 minutos

### OpÃ§Ã£o B: Com Views
- Inclui as 4 views auxiliares
- Total: 16 tabelas/views
- Tempo estimado: 7-12 minutos

## ğŸ“Š Estrutura de cada Data Schema

Cada arquivo `.md` gerado conterÃ¡:

```markdown
# Data Schema: niat.argos_benchmark_setorial

## DESCRIBE FORMATTED
- Estrutura completa da tabela
- Tipos de dados de cada coluna
- Propriedades do Hive/Impala
- LocalizaÃ§Ã£o no HDFS

## SAMPLE DATA (LIMIT 10)
- 10 primeiros registros
- Valores reais
- FormataÃ§Ã£o em tabela

## InformaÃ§Ãµes Adicionais
- Total de colunas
- Lista de colunas
- Quantidade de registros
```

## ğŸ” Comandos SQL Executados

Para cada tabela, o script executa:

```sql
-- 1. Obter estrutura
DESCRIBE FORMATTED schema.tabela;

-- 2. Obter dados de exemplo
SELECT * FROM schema.tabela LIMIT 10;
```

## ğŸ› ï¸ PersonalizaÃ§Ã£o

### Alterar quantidade de registros de exemplo

No notebook, modifique a linha:

```python
df_sample = executar_select_sample(spark, tabela, limit=10)
```

Para:

```python
df_sample = executar_select_sample(spark, tabela, limit=50)  # 50 registros
```

### Adicionar novas tabelas

No notebook, edite as listas na CÃ©lula 2:

```python
TABELAS_ORIGINAIS = [
    "usr_sat_ods.vw_ods_decl_dime",
    "usr_sat_ods.vw_ods_contrib",
    "usr_sat_ods.vw_ods_pagamento",
    "usr_sat_ods.nova_tabela"  # Adicione aqui
]
```

### Alterar diretÃ³rio de saÃ­da

```python
OUTPUT_DIR = "./meu-diretorio-schemas"  # Mude aqui
```

## âœ… Checklist de ExecuÃ§Ã£o

- [ ] Abrir Jupyter Notebook no ambiente correto
- [ ] Executar cÃ©lula 1 (Spark Session) - deve aparecer âœ…
- [ ] Executar cÃ©lula 2 (Carregar funÃ§Ãµes) - deve aparecer âœ…
- [ ] Executar cÃ©lula 3 (Gerar schemas) - aguardar conclusÃ£o
- [ ] Verificar mensagem "GERAÃ‡ÃƒO CONCLUÃDA"
- [ ] Executar cÃ©lula 4 para listar arquivos
- [ ] Conferir pasta `./data-schema/`

## ğŸ› Troubleshooting

### Erro: "SessÃ£o Spark nÃ£o encontrada"
**SoluÃ§Ã£o:** Execute a cÃ©lula 1 primeiro para configurar o Spark

### Erro: "Table not found"
**SoluÃ§Ã£o:** Verifique se a tabela existe no banco de dados
```python
spark.sql("SHOW TABLES IN niat").show()
```

### Erro: "Permission denied"
**SoluÃ§Ã£o:** Verifique permissÃµes do diretÃ³rio
```bash
chmod 755 data-schema
```

### Processo muito lento
**SoluÃ§Ã£o:** Execute em horÃ¡rio de menor uso do cluster ou reduza o LIMIT

## ğŸ“ˆ PrÃ³ximos Passos

ApÃ³s gerar os schemas:

1. **Revisar os arquivos** gerados em `./data-schema/`
2. **Adicionar ao Git** para versionamento
3. **Compartilhar** com a equipe para documentaÃ§Ã£o
4. **Atualizar** quando houver mudanÃ§as nas tabelas

## ğŸ”„ Quando Re-executar

Execute novamente quando:
- Adicionar novas tabelas ao projeto
- Modificar estrutura de tabelas existentes
- Preparar onboarding de novos membros
- Criar apresentaÃ§Ãµes tÃ©cnicas
- Atualizar documentaÃ§Ã£o

## ğŸ’¡ Dicas

1. **Versionamento:** Commit os schemas no Git junto com mudanÃ§as nas tabelas

2. **DocumentaÃ§Ã£o:** Use os schemas em PRs quando modificar estruturas

3. **Performance:** Se demorar muito, gere primeiro apenas as tabelas mais importantes

4. **ColaboraÃ§Ã£o:** Compartilhe a pasta `data-schema/` com desenvolvedores

5. **Backup:** Os arquivos sÃ£o texto simples, fÃ¡cil de fazer backup

---

## ğŸ“ Suporte

Se tiver problemas:
1. Verifique se a sessÃ£o Spark estÃ¡ ativa
2. Confirme acesso Ã s tabelas no Hive/Impala
3. Valide permissÃµes de escrita no diretÃ³rio

---

**Pronto para comeÃ§ar? Abra o notebook `GERAR_DATA_SCHEMAS.ipynb` e execute!** ğŸš€

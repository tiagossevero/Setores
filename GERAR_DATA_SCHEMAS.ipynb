{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# üìã GERADOR DE DATA-SCHEMAS - ARGOS SETORES\n",
    "\n",
    "Este notebook gera automaticamente os arquivos de data-schema para todas as tabelas do projeto.\n",
    "\n",
    "**O que ser√° gerado:**\n",
    "- `DESCRIBE FORMATTED` para cada tabela\n",
    "- `SELECT * FROM ... LIMIT 10` para cada tabela\n",
    "- Arquivos markdown organizados em `./data-schema/`\n",
    "\n",
    "**Tabelas processadas:**\n",
    "- 3 tabelas originais (ODS)\n",
    "- 9 tabelas intermedi√°rias (NIAT)\n",
    "- 4 views auxiliares (opcional)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1",
   "metadata": {},
   "source": [
    "## üîß PASSO 1: Configurar Sess√£o Spark\n",
    "\n",
    "Execute esta c√©lula primeiro para configurar a sess√£o Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-spark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/poc\")\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/plugins\")\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/dags\")\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import date\n",
    "\n",
    "from utils import spark_utils_session as utils\n",
    "from hooks.hdfs.hdfs_helper import HdfsHelper\n",
    "from jobs.job_base_config import BaseETLJobClass\n",
    "\n",
    "import poc_helper\n",
    "poc_helper.load_env(\"PROD\")\n",
    "\n",
    "def get_session(profile: str, dynamic_allocation_enabled: bool = True) -> utils.DBASparkAppSession:\n",
    "    \"\"\"Generates DBASparkAppSession.\"\"\"\n",
    "    app_name = \"tsevero_data_schema_generator\"\n",
    "    \n",
    "    spark_builder = (utils.DBASparkAppSession\n",
    "                     .builder\n",
    "                     .setAppName(app_name)\n",
    "                     .usingProcessProfile(profile)\n",
    "                    )\n",
    "    \n",
    "    if dynamic_allocation_enabled:\n",
    "        spark_builder.autoResourceManagement()\n",
    "\n",
    "    return spark_builder.build()\n",
    "\n",
    "session = get_session(profile='efd_t2')\n",
    "spark = session.sparkSession\n",
    "\n",
    "print(\"‚úÖ Sess√£o Spark configurada com sucesso!\")\n",
    "print(f\"App Name: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2",
   "metadata": {},
   "source": [
    "## üì¶ PASSO 2: Carregar Script Gerador\n",
    "\n",
    "Carrega as fun√ß√µes do gerador de data-schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURA√á√ÉO\n",
    "# =============================================================================\n",
    "\n",
    "TABELAS_ORIGINAIS = [\n",
    "    \"usr_sat_ods.vw_ods_decl_dime\",\n",
    "    \"usr_sat_ods.vw_ods_contrib\",\n",
    "    \"usr_sat_ods.vw_ods_pagamento\"\n",
    "]\n",
    "\n",
    "TABELAS_INTERMEDIARIAS = [\n",
    "    \"niat.argos_benchmark_setorial\",\n",
    "    \"niat.argos_benchmark_setorial_porte\",\n",
    "    \"niat.argos_empresas\",\n",
    "    \"niat.argos_pagamentos_empresa\",\n",
    "    \"niat.argos_empresa_vs_benchmark\",\n",
    "    \"niat.argos_evolucao_temporal_setor\",\n",
    "    \"niat.argos_evolucao_temporal_empresa\",\n",
    "    \"niat.argos_anomalias_setoriais\",\n",
    "    \"niat.argos_alertas_empresas\"\n",
    "]\n",
    "\n",
    "VIEWS_AUXILIARES = [\n",
    "    \"niat.vw_dashboard_setores\",\n",
    "    \"niat.vw_dashboard_empresas\",\n",
    "    \"niat.vw_analise_volatilidade\",\n",
    "    \"niat.vw_relacao_icms_pagamentos\"\n",
    "]\n",
    "\n",
    "OUTPUT_DIR = \"./data-schema\"\n",
    "\n",
    "# =============================================================================\n",
    "# FUN√á√ïES\n",
    "# =============================================================================\n",
    "\n",
    "def criar_diretorio_output(base_dir):\n",
    "    \"\"\"Cria estrutura de diret√≥rios para os data-schemas.\"\"\"\n",
    "    Path(base_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(f\"{base_dir}/originais\").mkdir(exist_ok=True)\n",
    "    Path(f\"{base_dir}/intermediarias\").mkdir(exist_ok=True)\n",
    "    Path(f\"{base_dir}/views\").mkdir(exist_ok=True)\n",
    "    print(f\"‚úÖ Diret√≥rios criados em: {base_dir}\")\n",
    "\n",
    "\n",
    "def formatar_nome_arquivo(tabela_completa):\n",
    "    \"\"\"Converte nome da tabela para nome de arquivo.\"\"\"\n",
    "    nome_tabela = tabela_completa.split('.')[-1]\n",
    "    return f\"{nome_tabela}.md\"\n",
    "\n",
    "\n",
    "def executar_describe_formatted(spark, tabela):\n",
    "    \"\"\"Executa DESCRIBE FORMATTED e retorna resultado como Pandas DataFrame.\"\"\"\n",
    "    try:\n",
    "        print(f\"  üìã Executando DESCRIBE FORMATTED {tabela}...\")\n",
    "        df = spark.sql(f\"DESCRIBE FORMATTED {tabela}\")\n",
    "        return df.toPandas()\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå ERRO ao descrever {tabela}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def executar_select_sample(spark, tabela, limit=10):\n",
    "    \"\"\"Executa SELECT * LIMIT e retorna resultado como Pandas DataFrame.\"\"\"\n",
    "    try:\n",
    "        print(f\"  üìä Executando SELECT * FROM {tabela} LIMIT {limit}...\")\n",
    "        df = spark.sql(f\"SELECT * FROM {tabela} LIMIT {limit}\")\n",
    "        return df.toPandas()\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå ERRO ao fazer SELECT de {tabela}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def gerar_markdown_dataschema(tabela, df_describe, df_sample):\n",
    "    \"\"\"Gera conte√∫do markdown do data-schema.\"\"\"\n",
    "\n",
    "    nome_tabela = tabela.split('.')[-1]\n",
    "    schema_nome = tabela.split('.')[0]\n",
    "\n",
    "    md = f\"\"\"# Data Schema: {tabela}\n",
    "\n",
    "**Gerado em:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "---\n",
    "\n",
    "## üìã DESCRIBE FORMATTED\n",
    "\n",
    "```sql\n",
    "DESCRIBE FORMATTED {tabela};\n",
    "```\n",
    "\n",
    "### Resultado:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    if df_describe is not None and not df_describe.empty:\n",
    "        md += \"```\\n\"\n",
    "        md += df_describe.to_string(index=False, max_colwidth=100)\n",
    "        md += \"\\n```\\n\"\n",
    "    else:\n",
    "        md += \"_Nenhum resultado dispon√≠vel_\\n\"\n",
    "\n",
    "    md += f\"\"\"\n",
    "\n",
    "---\n",
    "\n",
    "## üìä SAMPLE DATA (LIMIT 10)\n",
    "\n",
    "```sql\n",
    "SELECT * FROM {tabela} LIMIT 10;\n",
    "```\n",
    "\n",
    "### Resultado:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    if df_sample is not None and not df_sample.empty:\n",
    "        md += \"```\\n\"\n",
    "        md += df_sample.to_string(index=False, max_colwidth=50)\n",
    "        md += \"\\n```\\n\"\n",
    "\n",
    "        md += f\"\"\"\n",
    "\n",
    "### Informa√ß√µes Adicionais:\n",
    "\n",
    "- **Total de colunas:** {len(df_sample.columns)}\n",
    "- **Colunas:** {', '.join(df_sample.columns.tolist())}\n",
    "- **Registros retornados:** {len(df_sample)}\n",
    "\n",
    "\"\"\"\n",
    "    else:\n",
    "        md += \"_Nenhum resultado dispon√≠vel_\\n\"\n",
    "\n",
    "    md += \"\"\"\n",
    "---\n",
    "\n",
    "## üìù Notas\n",
    "\n",
    "- Este arquivo foi gerado automaticamente\n",
    "- Utilize este schema como refer√™ncia para an√°lises e desenvolvimento\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    return md\n",
    "\n",
    "\n",
    "def salvar_arquivo(conteudo, caminho):\n",
    "    \"\"\"Salva conte√∫do em arquivo.\"\"\"\n",
    "    try:\n",
    "        with open(caminho, 'w', encoding='utf-8') as f:\n",
    "            f.write(conteudo)\n",
    "        print(f\"  ‚úÖ Salvo: {caminho}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå ERRO ao salvar {caminho}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "print(\"‚úÖ Fun√ß√µes carregadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3",
   "metadata": {},
   "source": [
    "## üöÄ PASSO 3: Executar Gera√ß√£o\n",
    "\n",
    "### Op√ß√µes:\n",
    "\n",
    "**Op√ß√£o A:** Gerar apenas tabelas originais e intermedi√°rias (recomendado)\n",
    "\n",
    "**Op√ß√£o B:** Gerar incluindo views auxiliares\n",
    "\n",
    "Execute a c√©lula correspondente √† op√ß√£o desejada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "execute-option-a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OP√á√ÉO A: SEM VIEWS (RECOMENDADO)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GERADOR DE DATA-SCHEMAS - ARGOS SETORES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Iniciado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "criar_diretorio_output(OUTPUT_DIR)\n",
    "\n",
    "total = 0\n",
    "sucesso = 0\n",
    "erros = 0\n",
    "\n",
    "# Processar tabelas originais\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROCESSANDO TABELAS ORIGINAIS (ODS)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for tabela in TABELAS_ORIGINAIS:\n",
    "    total += 1\n",
    "    print(f\"\\n[{total}] Processando: {tabela}\")\n",
    "    \n",
    "    df_desc = executar_describe_formatted(spark, tabela)\n",
    "    df_sample = executar_select_sample(spark, tabela, limit=10)\n",
    "    \n",
    "    if df_desc is not None or df_sample is not None:\n",
    "        md_content = gerar_markdown_dataschema(tabela, df_desc, df_sample)\n",
    "        nome_arquivo = formatar_nome_arquivo(tabela)\n",
    "        caminho = f\"{OUTPUT_DIR}/originais/{nome_arquivo}\"\n",
    "        \n",
    "        if salvar_arquivo(md_content, caminho):\n",
    "            sucesso += 1\n",
    "        else:\n",
    "            erros += 1\n",
    "    else:\n",
    "        erros += 1\n",
    "        print(f\"  ‚ö†Ô∏è Pulando {tabela} (sem dados)\")\n",
    "\n",
    "# Processar tabelas intermedi√°rias\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROCESSANDO TABELAS INTERMEDI√ÅRIAS (NIAT)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for tabela in TABELAS_INTERMEDIARIAS:\n",
    "    total += 1\n",
    "    print(f\"\\n[{total}] Processando: {tabela}\")\n",
    "    \n",
    "    df_desc = executar_describe_formatted(spark, tabela)\n",
    "    df_sample = executar_select_sample(spark, tabela, limit=10)\n",
    "    \n",
    "    if df_desc is not None or df_sample is not None:\n",
    "        md_content = gerar_markdown_dataschema(tabela, df_desc, df_sample)\n",
    "        nome_arquivo = formatar_nome_arquivo(tabela)\n",
    "        caminho = f\"{OUTPUT_DIR}/intermediarias/{nome_arquivo}\"\n",
    "        \n",
    "        if salvar_arquivo(md_content, caminho):\n",
    "            sucesso += 1\n",
    "        else:\n",
    "            erros += 1\n",
    "    else:\n",
    "        erros += 1\n",
    "        print(f\"  ‚ö†Ô∏è Pulando {tabela} (sem dados)\")\n",
    "\n",
    "# Relat√≥rio final\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RELAT√ìRIO FINAL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total de tabelas processadas: {total}\")\n",
    "print(f\"‚úÖ Sucesso: {sucesso}\")\n",
    "print(f\"‚ùå Erros: {erros}\")\n",
    "print(f\"\\nüìÅ Arquivos salvos em: {OUTPUT_DIR}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ GERA√á√ÉO CONCLU√çDA!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "execute-option-b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OP√á√ÉO B: COM VIEWS AUXILIARES\n",
    "# =============================================================================\n",
    "# Descomente e execute esta c√©lula se quiser incluir as views auxiliares\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 80)\n",
    "# print(\"PROCESSANDO VIEWS AUXILIARES\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# for tabela in VIEWS_AUXILIARES:\n",
    "#     total += 1\n",
    "#     print(f\"\\n[{total}] Processando: {tabela}\")\n",
    "    \n",
    "#     df_desc = executar_describe_formatted(spark, tabela)\n",
    "#     df_sample = executar_select_sample(spark, tabela, limit=10)\n",
    "    \n",
    "#     if df_desc is not None or df_sample is not None:\n",
    "#         md_content = gerar_markdown_dataschema(tabela, df_desc, df_sample)\n",
    "#         nome_arquivo = formatar_nome_arquivo(tabela)\n",
    "#         caminho = f\"{OUTPUT_DIR}/views/{nome_arquivo}\"\n",
    "        \n",
    "#         if salvar_arquivo(md_content, caminho):\n",
    "#             sucesso += 1\n",
    "#         else:\n",
    "#             erros += 1\n",
    "#     else:\n",
    "#         erros += 1\n",
    "#         print(f\"  ‚ö†Ô∏è Pulando {tabela} (sem dados)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4",
   "metadata": {},
   "source": [
    "## üìä PASSO 4: Verificar Resultados\n",
    "\n",
    "Liste os arquivos gerados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"üìÅ ARQUIVOS GERADOS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for root, dirs, files in os.walk(OUTPUT_DIR):\n",
    "    level = root.replace(OUTPUT_DIR, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        print(f\"{subindent}üìÑ {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "footer",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Conclu√≠do!\n",
    "\n",
    "Os arquivos de data-schema foram gerados em:\n",
    "\n",
    "```\n",
    "./data-schema/\n",
    "‚îú‚îÄ‚îÄ originais/          # Tabelas ODS (usr_sat_ods.*)\n",
    "‚îú‚îÄ‚îÄ intermediarias/     # Tabelas NIAT (niat.argos_*)\n",
    "‚îî‚îÄ‚îÄ views/              # Views auxiliares (niat.vw_*)\n",
    "```\n",
    "\n",
    "Cada arquivo cont√©m:\n",
    "- DESCRIBE FORMATTED completo\n",
    "- SELECT com 10 registros de exemplo\n",
    "- Informa√ß√µes sobre colunas e tipos\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
